####【任务1 - 线性回归算法梳理】

1. ######机器学习的一些概念 有监督、无监督、泛化能力、过拟合欠拟合(方差和偏差以及各自解决办法)、交叉验证

   监督学习：通过已有的一部分输入数据与输出数据之间的对应关系，生成一个函数，将输入映射到合适的输出，例如分类。
   无监督学习：直接对输入数据集进行建模，例如聚类。

   泛化能力：指一个机器学习算法对于没有见过的样本的识别能力。我们也叫做举一反三的能力，或者叫做学以致用的能力。

   过拟合：模型的复杂度要高于实际的问题，所以就会导致模型死记硬背的记住，而没有理解背后的规律。

   欠拟合：模型的复杂度较低，没法很好的学习到数据背后的规律。

   交叉验证：交叉验证的思想是重复使用数据，把给定的数据进行切分，在此基础上不断的训练、验证以及模型选择

2. ######线性回归的原理

   使用线性回归是在数据所在的N维空间中找到一条线来描述这些数据的规律，因此才叫线性回归。这个过程称为拟合，这条线成为拟合线。

3. ######线性回归损失函数、代价函数、目标函数

   1. 损失函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%7C+y_i-f%28x_i%29+%5Cright%7C) ，一般是针对单个样本 i
   2. 代价函数 ![[公式]](https://www.zhihu.com/equation?tex=1%2FN.%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7B%5Cleft%7C+y_i-f%28x_i%29+%5Cright%7C%7D) , 一般是针对总体
   3. 目标函数 ![[公式]](https://www.zhihu.com/equation?tex=1%2FN.%5Csum_%7Bi%3D1%7D%5E%7BN%7D%7B%5Cleft%7C+y_i-f%28x_i%29+%5Cright%7C%7D+%2B+%E6%AD%A3%E5%88%99%E5%8C%96%E9%A1%B9)

4. ######优化方法(梯度下降法、牛顿法、拟牛顿法等)

   梯度下降：梯度下降是用来求函数最小值的算法，因此我们使用梯度下降算法来求出代价函数J(θ_0，θ_1 )的最小值。梯度下降是用来求函数最小值的算法，因此我们使用梯度下降算法来求出代价函数J(θ_0，θ_1 )的最小值。

   牛顿法：牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。

   拟牛顿法：拟牛顿法的本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。拟牛顿法和最速下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于最速下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

5. ######线性回归的评估指标

   平均绝对误差和平均绝对误差

6. ######sklearn参数详解

   **fit_intercept** : boolean, optional, default True

   是否有截据，如果没有则直线过原点。

   **normalize:**是否将数据归一化。

   **copy_X**:默认为True，当为True时，X会被copied,否则X将会被覆写。（这一参数的具体作用没明白，求大神指教了）

   **n_jobs**:默认值为1。计算时使用的核数。